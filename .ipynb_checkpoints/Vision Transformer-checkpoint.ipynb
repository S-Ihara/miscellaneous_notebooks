{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530943c3",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)の論文が発表されてからTransformerの勢いは自然言語処理に留まらず様々な分野で高い性能を叩き出している。画像処理分野でもTransformerの波は押し寄せており、[Vision Tranformer](https://arxiv.org/abs/2010.11929)という2021年にICLRで発表された論文ではImagenetのクラス分類においてトップクラスの性能を出せるTransformerベースのモデルを提案している。\n",
    "\n",
    "ここではそのVision TransformerのモデルをベースにTransformerについて学びを深める。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd043ee9",
   "metadata": {},
   "source": [
    "### Aim\n",
    "- Vision Transformerを通じてTransformerについて学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459343b",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "- [Vision Transformer入門](https://gihyo.jp/book/2022/978-4-297-13058-9)\n",
    "    - 分かりやすくViTやViT周りの様々な論文なども紹介されている。  \n",
    "\n",
    "todo 適宜追加していく、何を参考にしたか何が良かったかもう忘れちった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e09c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b79f651",
   "metadata": {},
   "source": [
    "今回はpytorchを使用してスクラッチ実装をする\n",
    "\n",
    "Transformerでは情報を*トークン*と呼ばれる特徴量ベクトル単位で扱う。例えば自然言語処理では「This is my pen.」という文は単語ごとに分けられ[^1] 、this, is, my, pen, ., の5つのトークンとして扱う。1つのトークンは適当な次元のベクトルとして表される。  \n",
    "画像処理においてVision Transformerでは固定長の画像パッチに分割しその1つの画像パッチをトークンとする。\n",
    "<!-- todo いい感じの図を入れる -->\n",
    "\n",
    "どうトークンにするかは学習可能なパラメータによりモデルに行ってもらう。  \n",
    "具体的には全結合層によりトークンに変換する。\n",
    "\n",
    "\n",
    "[^1:]正確には必ずしも単語になるわけではない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ceb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "# 画像を16x16 pixelのパッチに分けそれぞれをtransformerで扱うためのトークンへと変換する\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImagePatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    input: image:torch.Tensor[n,c,h,w]\n",
    "    output: embedding_vector:torch.Tensor[n,p,dim]\n",
    "    \n",
    "    nはバッチサイズ、chwは画像のカラーチャネル、高さ、幅であり、pはトークン数（＝パッチの個数）\n",
    "    dimは埋め込みベクトルの長さ（ハイパーパラメータ）\n",
    "    \"\"\"\n",
    "    def __init__(self,image_size=224,patch_size=16,in_channel=3,embedding_dim=768):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            image_size:Union[int, tuple] 画像の高さと幅\n",
    "            patch_size:int 1画像パッチのピクセル幅\n",
    "            in_channel:int 入力画像のチャネル数\n",
    "            embeedding_dim:int トークン（埋め込みベクトル）の長さ\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        image_size = self._pair(image_size) # if int -> tuple\n",
    "        patch_size = self._pair(patch_size)\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (image_size[0]//patch_size[0], image_size[1]//patch_size[1])\n",
    "        self.num_patches = self.grid_size[0]*self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channel,embedding_dim,patch_size,patch_size)\n",
    "        #self.normalize = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        assert h == self.image_size[0] and w == self.image_size[1], f'Input image size ({h}*{w}) doesn\\'t match model ({self.image_size[0]}*{self.image_size[1]}).'\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1,2) # (N,C,H,W) -> (B,P,C)\n",
    "        #x = self.normalize(x)\n",
    "        return x\n",
    "\n",
    "    def _pair(self,x):\n",
    "        \"\"\"\n",
    "        utils\n",
    "        return a tuple if x is int \n",
    "        \"\"\"\n",
    "        return t if isinstance(t, tuple) else (t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9681a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01265f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8abe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea50d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
