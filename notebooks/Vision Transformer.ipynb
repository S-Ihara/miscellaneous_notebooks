{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530943c3",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)の論文が発表されてからTransformerの勢いは自然言語処理に留まらず様々な分野で高い性能を叩き出している。画像処理分野でもTransformerの波は押し寄せており、[Vision Tranformer](https://arxiv.org/abs/2010.11929)という2021年にICLRで発表された論文ではImagenetのクラス分類においてトップクラスの性能を出せるTransformerベースのモデルを提案している。\n",
    "\n",
    "ここではそのVision Transformerのモデルをpytorchを使ってスクラッチ実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459343b",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "- [Vision Transformer入門(本)](https://gihyo.jp/book/2022/978-4-297-13058-9)\n",
    "    - 分かりやすくViTやViT周りの様々な論文なども紹介されている\n",
    "- [huffingfaceのgihub](https://github.com/huggingface/transformers/tree/main/src/transformers/models/vit)\n",
    "    - コードの参考など\n",
    "    - 比較的読みやすい、、はず\n",
    "- [【深層学習】Transformer - Multi-Head Attentionを理解してやろうじゃないの【ディープラーニングの世界vol.28】](https://www.youtube.com/watch?v=50XvMaWhiTY)\n",
    "    - ViTではなく本家Transformerの解説動画\n",
    "    - Transformerの内部の処理（特にMulti-Head Attention）が分かりやすく解説されている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80121062",
   "metadata": {},
   "source": [
    "### input layer\n",
    "画像をパッチ分割してトークン（ベクトル）に埋め込む。  \n",
    "実装としては画像をパッチ分割 -> 平坦化 -> 全結合層の一連の流れは畳み込み層を使って簡単に実装できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80da0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルに必要なライブラリ\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36ceb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "# 画像を16x16 pixelのパッチに分けそれぞれをtransformerで扱うためのトークンへと変換する\n",
    "\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "\n",
    "class ImagePatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    input: image:torch.Tensor[n,c,h,w]\n",
    "    output: embedding_vector:torch.Tensor[n,p,dim]\n",
    "    \n",
    "    nはバッチサイズ、chwは画像のカラーチャネル、高さ、幅であり、pはトークン数（＝パッチの個数）\n",
    "    dimは埋め込みベクトルの長さ（ハイパーパラメータ）\n",
    "    \"\"\"\n",
    "    def __init__(self,image_size=224,patch_size=16,in_channel=3,embedding_dim=768):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            image_size:Union[int, tuple] 画像の高さと幅\n",
    "            patch_size:int 1画像パッチのピクセル幅\n",
    "            in_channel:int 入力画像のチャネル数\n",
    "            embeedding_dim:int トークン（埋め込みベクトル）の長さ\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        image_size = self._pair(image_size) # if int -> tuple\n",
    "        patch_size = self._pair(patch_size)\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (image_size[0]//patch_size[0], image_size[1]//patch_size[1])\n",
    "        self.num_patches = self.grid_size[0]*self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channel,embedding_dim,patch_size,patch_size)\n",
    "        #self.normalize = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x:torch.Tensor[b,c,h,w]\n",
    "        -> torch.Tensor[b,p,dim]\n",
    "        \"\"\"\n",
    "        n,c,h,w = x.shape\n",
    "        assert h == self.image_size[0] and w == self.image_size[1], f'Input image size ({h}*{w}) doesn\\'t match model ({self.image_size[0]}*{self.image_size[1]}).'\n",
    "\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1,2) # (N,C,H,W) -> (B,P,C)\n",
    "        #x = self.normalize(x)\n",
    "        return x\n",
    "\n",
    "    def _pair(self,x):\n",
    "        \"\"\"\n",
    "        util function\n",
    "        return a tuple if x is int \n",
    "        \"\"\"\n",
    "        return x if isinstance(x, collections.abc.Iterable) else (x, x)\n",
    "        #return x if isinstance(x, tuple) else (x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517a957",
   "metadata": {},
   "source": [
    "### multihead attention\n",
    "multiheadに分割する関係上shapeが分かりづらくなるのでいちいち確認していくのが良い。  \n",
    "実装上分割してるためmultihead attentionの解説でもちょんぎるような解説がなされているが直感的には分割しているのではなく、小さい複数のベクトルに埋め込んで処理を行っているという方が分かりやすい気がする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad161902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    input: embedding_vector:torch.Tensor[n,p,dim]\n",
    "    output: embedding_vector:torch.Tensor[n,p,dim]\n",
    "    \"\"\"\n",
    "    def __init__(self,dim,num_heads=8,qkv_bias=True,dropout=0.):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim: int トークン（埋め込みベクトル）の長さ\n",
    "            num_heads: int マルチヘッドの数\n",
    "            qkv_bias: bool query,key,valueに埋め込む際の全結合層のバイアス\n",
    "            dropout: float dropoutの確率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        assert dim % num_heads == 0, f\"The hidden size {dim} is not a multiple of the number of head attention\"\n",
    "        self.hidden_dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(dim,dim,bias=qkv_bias)\n",
    "        self.key = nn.Linear(dim,dim,bias=qkv_bias)\n",
    "        self.value = nn.Linear(dim,dim,bias=qkv_bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size,num_patches,_ = x.size()\n",
    "\n",
    "        # query,key,value (B,P,C) -> (B,P,C)\n",
    "        print(\"x\",x.shape)\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # マルチヘッドに分割\n",
    "        # (B,P,C) -> (B,Nh,P,Dh)\n",
    "        multihead_qkv_shape = torch.Size([batch_size, num_patches, self.num_heads, self.head_dim])\n",
    "        qs = q.view(multihead_qkv_shape)\n",
    "        qs = qs.permute(0, 2, 1, 3)\n",
    "        ks = k.view(multihead_qkv_shape)\n",
    "        ks = ks.permute(0, 2, 1, 3)\n",
    "        ks_T = ks.transpose(2,3)\n",
    "        vs = v.view(multihead_qkv_shape)\n",
    "        vs = vs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # (B,Nh,P,Dh) @ (B,Nh,Dh,P) -> (B,Nh,P,P)\n",
    "        scaled_dot_product = qs@ks_T / np.sqrt(self.head_dim) \n",
    "        self_attention = nn.functional.softmax(scaled_dot_product,dim=-1)\n",
    "        self_attention = self.dropout(self_attention)\n",
    "        \n",
    "        # (B,Nh,P,P) @ (B,Nh,P,Dh) -> (B,Nh,P,Dh)\n",
    "        context_layer = self_attention@vs\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous().reshape(batch_size,num_patches,self.hidden_dim)\n",
    "        # (B,P,Nh,Dh) -> (B,P,C)\n",
    "        \n",
    "        out = self.projection(context_layer)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ViTFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    input: embedding_vector:torch.Tensor[n,p,dim]\n",
    "    output: embedding_vector:torch.Tensor[n,p,dim]\n",
    "    \"\"\"\n",
    "    def __init__(self,dim,hidden_dim=768*4,activation=nn.GELU(),dropout=0.):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim: int トークン（埋め込みベクトル）の長さ\n",
    "            hidden_dim: FeedForwardネットワークでの中間層のベクトルの長さ\n",
    "                        慣例的にdim*4が使われている(少なくともViTでは)\n",
    "                        本家では違うかも\n",
    "            activation: torch.nn.modules.activation 活性化関数\n",
    "            dropout: float dropoutの確率    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim,hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim,dim)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=768,\n",
    "        hidden_dim=768*4,\n",
    "        num_heads=12,\n",
    "        activation=nn.GELU(),\n",
    "        qkv_bias=True,\n",
    "        dropout=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(dim,num_heads,qkv_bias,dropout)\n",
    "        self.ff   = ViTFeedForward(dim,hidden_dim,activation,dropout)\n",
    "        self.ln = nn.LayerNorm(dim,eps=1e-10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        input: torch.Tensor[n,p,dim]\n",
    "        output; torch.Tensor[n,p,dim]\n",
    "        \"\"\"\n",
    "        z = self.ln(x)\n",
    "        z = self.mhsa(z)\n",
    "        x = x + z\n",
    "        z = self.ln(x)\n",
    "        z = self.ff(x)\n",
    "        out = x + z  \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fcb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    todo attentionも取り出したければ取り出せるように\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=768,\n",
    "        hidden_dim=768*4,\n",
    "        num_heads=12,\n",
    "        activation=nn.GELU(),\n",
    "        qkv_bias=True,\n",
    "        dropout=0.,\n",
    "        num_blocks=8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            num_blocks: int ViTBlockの総数\n",
    "            他は割愛\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([ViTBlock(\n",
    "            dim,hidden_dim,num_heads,activation,qkv_bias,dropout\n",
    "        ) for _ in range(num_blocks)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            x = layer_module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf03283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        in_channel=3,\n",
    "        dim=768,\n",
    "        hidden_dim=768*4,\n",
    "        num_heads=12,\n",
    "        activation=nn.GELU(),\n",
    "        num_blocks=8,\n",
    "        qkv_bias=True,\n",
    "        dropout=0.,\n",
    "        num_classes=1000,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            image_size: int 入力画像の解像度\n",
    "            patch_size: int パッチ分割の際の1パッチのピクセル数\n",
    "            in_channel: int 入力画像のチャネル数\n",
    "            dim: int トークン（埋め込みベクトル）の長さ\n",
    "            hidden_dim: int FeedForward層でのベクトルの長さ\n",
    "            num_heads: int マルチヘッドの数\n",
    "            activation: torch.nn.modules.activation 活性化関数\n",
    "            num_blocks: int ViTBlockの総数\n",
    "            qkv_bias: bool query,key,valueに埋め込む際の全結合層のバイアス\n",
    "            dropout: float dropoutの確率\n",
    "            num_classes: int 出力次元数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # input layer\n",
    "        self.patch_embedding = ImagePatchEmbedding(image_size,patch_size,in_channel,dim)\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1,1,dim))) # クラストークン（学習可能なパラメータ）\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(size=(1,num_patches+1,dim))) #　位置埋め込み（学習可能なパラメータ）\n",
    "        \n",
    "        # vit encoder \n",
    "        self.encoder = ViTEncoder(dim,hidden_dim,num_heads,activation,qkv_bias,dropout,num_blocks)\n",
    "        \n",
    "        # mlp head\n",
    "        self.ln = nn.LayerNorm(dim,eps=1e-10)\n",
    "        self.head = nn.Linear(dim,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0],-1,-1)\n",
    "        x = torch.cat((cls_token,x),dim=1) # (B,num_patches+1,embedding_dim)\n",
    "        x = x + self.positional_embedding\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.index_select(x,1,torch.tensor(0,device=x.device))\n",
    "        x = x.squeeze(1)\n",
    "        x = self.ln(x)\n",
    "        out = self.head(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8f24d",
   "metadata": {},
   "source": [
    "入力としてimagenetの画像、出力として1000クラス分類を考える。  \n",
    "出力が1000次元になっているのが確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b1451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 197, 768])\n",
      "x torch.Size([1, 197, 768])\n",
      "x torch.Size([1, 197, 768])\n",
      "x torch.Size([1, 197, 768])\n",
      "x torch.Size([1, 197, 768])\n",
      "x torch.Size([1, 197, 768])\n",
      "x torch.Size([1, 197, 768])\n",
      "x torch.Size([1, 197, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViT = VisionTransformer()\n",
    "image = torch.randn(size=(1,3,224,224))\n",
    "output = ViT(image)\n",
    "output.shape # [batch_size, num_classes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
